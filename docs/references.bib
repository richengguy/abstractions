@article{tian2021,
  author       = {Yingtao Tian and
                  David Ha},
  title        = {Modern Evolution Strategies for Creativity: Fitting Concrete Images
                  and Abstract Concepts},
  journal      = {CoRR},
  volume       = {abs/2109.08857},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.08857},
  eprinttype    = {arXiv},
  eprint       = {2109.08857},
  timestamp    = {Mon, 27 Sep 2021 15:21:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-08857.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{toklu2020,
author="Toklu, Nihat Engin
and Liskowski, Pawe{\l}
and Srivastava, Rupesh Kumar",
editor="B{\"a}ck, Thomas
and Preuss, Mike
and Deutz, Andr{\'e}
and Wang, Hao
and Doerr, Carola
and Emmerich, Michael
and Trautmann, Heike",
title="ClipUp: A Simple and Powerful Optimizer for Distribution-Based Policy Evolution",
booktitle="Parallel Problem Solving from Nature -- PPSN XVI",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="515--527",
abstract="Distribution-based search algorithms are a powerful approach for evolutionary reinforcement learning of neural network controllers. In these algorithms, gradients of the reward function with respect to the policy parameters are estimated using a population of solutions drawn from a search distribution, and then used for policy optimization with stochastic gradient ascent. A common choice is to use the Adam optimization algorithm for obtaining an adaptive behavior during gradient ascent, due to its success in a variety of supervised learning settings. As an alternative to Adam, we propose to enhance classical momentum-based gradient ascent with two simple-yet-effective techniques: gradient normalization and update clipping. We argue that the resulting optimizer called ClipUp (short for clipped updates) is a better choice for distribution-based policy evolution because its working principles are simple and easy to understand and its hyperparameters can be tuned more intuitively in practice. Moreover, it avoids the need to re-tune hyperparameters if the reward scale changes. Experiments show that ClipUp is competitive with Adam despite its simplicity and is effective at some of the most challenging continuous control benchmarks, including the Humanoid control task based on the Bullet physics simulator.",
isbn="978-3-030-58115-2",
url="https://link.springer.com/chapter/10.1007/978-3-030-58115-2_36",
}
